{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Web_Crawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcEKnr4iK9k8"
      },
      "source": [
        "# Crawler\n",
        "\n",
        "This notebook contains code structure for creating a crawler on single machine\n",
        "\n",
        "\n",
        "**Authored by:** Fatima Hasan\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaevDc0OK9lX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cdbefaf-3852-48c8-df5c-d45bb49d7ea2"
      },
      "source": [
        "!pip install tldextract\n",
        "!pip install pymongo\n",
        "!pip install bs4\n",
        "!pip install requests\n",
        "!pip install dnspython==2.0.0\n",
        "\n",
        "import os \n",
        "import random\n",
        "from queue import PriorityQueue, Queue\n",
        "import datetime\n",
        "from urllib.parse import urlsplit\n",
        "import urllib.robotparser as urobot\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import threading\n",
        "\n",
        "from pymongo import MongoClient\n",
        "import tldextract\n",
        "# Add any library to be imported here"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.10)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract) (3.0.12)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract) (1.5.1)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from requests-file>=1.4->tldextract) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (3.11.3)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: dnspython==2.0.0 in /usr/local/lib/python3.7/dist-packages (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GubSnV7K9lb"
      },
      "source": [
        "# Crawler Parameters\n",
        "BACKQUEUES= 3\n",
        "THREADS= BACKQUEUES*3\n",
        "FRONTQUEUES= 5\n",
        "WAITTIME= 15 ; # wait 15 seconds before fetching URLS from \n",
        "client = MongoClient(\"mongodb+srv://fatima123:fatima123@crawlercluster.r66a9.mongodb.net/information_retrieval?retryWrites=true&w=majority\")\n",
        "db = client.information_retrieval\n",
        "records = db.crawler_fetched_pages\n",
        "threadLock = threading.Lock()\n",
        "\n",
        "crawled_urls=[]\n",
        "# Add any other global parameters here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_MWdoQzK9lc"
      },
      "source": [
        "# FRONTIER\n",
        "\n",
        "Frontier uses the Mercator frontier design \n",
        "\n",
        "*prioritizer* function is a stub right now, it will return a random number  between 1 to f for given URL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GHC1cip9K9le"
      },
      "source": [
        "class frontier:\n",
        "# add the code for frontier here\n",
        "# should have functions __init__, get_URL, add_URLs, add_to_backqueue\n",
        "    def __init__(self, seed_urls):\n",
        "        '''\n",
        "        Initializer\n",
        "        '''\n",
        "        self.front_queues = [ Queue() for i in range(FRONTQUEUES) ]\n",
        "        self.back_queues = [ Queue() for i in range(BACKQUEUES) ]\n",
        "        self.heap = PriorityQueue()\n",
        "\n",
        "        self.add_URLs(seed_urls)\n",
        "        self.update_back_queues()\n",
        "\n",
        "        self.print_queues()\n",
        "\n",
        "      \n",
        "    def update_back_queues(self):\n",
        "      if self.are_frontqueues_empty() == False:\n",
        "        probabilities = []\n",
        "        sum_of_priorities = (FRONTQUEUES*(FRONTQUEUES+1))/2\n",
        "        for i in range(FRONTQUEUES):\n",
        "          probabilities.append((i+1)/sum_of_priorities)\n",
        "\n",
        "        while self.empty_backqueue_index() != -1 and self.are_frontqueues_empty() == False:\n",
        "          probability = random.random()\n",
        "          front_queue_number = 0\n",
        "          sum = 0\n",
        "          for i in range(FRONTQUEUES):\n",
        "            sum += probabilities[i]\n",
        "            if probability <= sum:\n",
        "              front_queue_number = i\n",
        "              break\n",
        "\n",
        "          if(self.front_queues[front_queue_number].empty() == False):\n",
        "            self.add_to_backqueue(self.front_queues[front_queue_number].get())\n",
        "\n",
        "    def are_frontqueues_empty(self):\n",
        "      for q in self.front_queues:\n",
        "        if q.empty() == False:\n",
        "          return False\n",
        "      return True\n",
        "\n",
        "    def are_backqueues_empty(self):\n",
        "      for q in self.back_queues:\n",
        "        if q.empty() == False:\n",
        "          return False\n",
        "      return True\n",
        "\n",
        "    def empty_backqueue_index(self):\n",
        "      for index, q in enumerate(self.back_queues):\n",
        "        if q.empty() == True:\n",
        "          return index\n",
        "      return -1\n",
        "        \n",
        "    def add_URLs(self,urls):\n",
        "      for url in urls:\n",
        "        self.front_queues[prioritizer(url,FRONTQUEUES)-1].put(url)\n",
        "      self.update_back_queues()\n",
        "\n",
        "    def add_to_backqueue(self,url):\n",
        "      added = 0\n",
        "      \n",
        "      empty_index = self.empty_backqueue_index()\n",
        "      if empty_index != -1:\n",
        "        domain = tldextract.extract(url).domain\n",
        "        for q in self.back_queues:\n",
        "          if q.empty() == False and tldextract.extract(q.queue[0]).domain == domain:\n",
        "            q.put(url)\n",
        "            added = 1\n",
        "            break\n",
        "\n",
        "        if added == 0:\n",
        "          self.back_queues[empty_index].put(url)\n",
        "          \n",
        "          exists = 0\n",
        "          for i in range(len(list(self.heap.queue))):\n",
        "            if self.heap.queue[i][1] == domain:\n",
        "              exists = 1\n",
        "          if exists == 0:\n",
        "            self.heap.put((datetime.datetime.now(), domain))\n",
        "\n",
        "    def print_queues(self):\n",
        "      print(\"FRONT QUEUES: \\n\")\n",
        "      for q in self.front_queues:\n",
        "        print(list(q.queue))\n",
        "\n",
        "      print(\"\\nBACK QUEUES: \\n\")\n",
        "      for q in self.back_queues:\n",
        "        print(list(q.queue))\n",
        "\n",
        "    def print_heap(self):\n",
        "      print(list(self.heap.queue))\n",
        "\n",
        "    def get_URL(self):\n",
        "      found = 0\n",
        "\n",
        "      if self.are_backqueues_empty() == True:\n",
        "        return \"\"\n",
        "\n",
        "      while found == 0:\n",
        "        element = self.heap.get()\n",
        "        domain = element[1]\n",
        "        time = element[0] + datetime.timedelta(0, WAITTIME)\n",
        "\n",
        "        while datetime.datetime.now() < element[0]:\n",
        "          pass\n",
        "\n",
        "        for q in self.back_queues:\n",
        "          if q.empty() == False and tldextract.extract(q.queue[0]).domain == domain:\n",
        "            self.heap.put((time,domain))\n",
        "            self.update_back_queues()\n",
        "            return q.get()\n",
        "\n",
        "    def exists_in_frontier(self, url):\n",
        "      for q in self.front_queues:\n",
        "        if url in list(q.queue):\n",
        "          return True\n",
        "      \n",
        "      for q in self.back_queues:\n",
        "        if url in list(q.queue):\n",
        "          return True\n",
        "    \n",
        "      return False\n",
        "  \n",
        "\n",
        "def prioritizer(URL,f):\n",
        "    \"\"\"\n",
        "    Take URL and returns priority from 1 to F\n",
        "    Right now it like a stub function. \n",
        "    It will return a random number from 1 to f for given inputs. \n",
        "    \"\"\"\n",
        "    return random.randint(1,f)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWR3CI9dK9lh"
      },
      "source": [
        "# FILTER URLS\n",
        "\n",
        "Filter the URLS that are in robots.txt files of server and the have been already processed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4QpcQMNK9li"
      },
      "source": [
        "# write code for filtering urls here \n",
        "def filter_urls(base_url, extracted_urls, frontier):\n",
        "  filtered_urls = []\n",
        "  url = urlsplit(base_url).netloc\n",
        "  url = 'https://' + url\n",
        "  try:\n",
        "    rp = urobot.RobotFileParser()\n",
        "    rp.set_url(url + \"/robots.txt\")\n",
        "    rp.read()\n",
        "    for new_url in extracted_urls:\n",
        "      new_url = requests.compat.urljoin(base_url, new_url)\n",
        "      if new_url != \"#\":\n",
        "        if tldextract.extract(new_url).domain == tldextract.extract(base_url).domain:\n",
        "          threadLock.acquire()\n",
        "          if rp.can_fetch(\"*\", new_url) and new_url not in crawled_urls and frontier.exists_in_frontier(new_url) == False:\n",
        "            filtered_urls.append(new_url)\n",
        "          threadLock.release()\n",
        "        else:\n",
        "          url2 = urlsplit(new_url).netloc\n",
        "          url2 = 'https://' + url2\n",
        "          try:\n",
        "            rp2 = urobot.RobotFileParser()\n",
        "            rp2.set_url(url2 + \"/robots.txt\")\n",
        "            rp2.read()\n",
        "\n",
        "            threadLock.acquire()\n",
        "            if rp2.can_fetch(\"*\", new_url) and new_url not in crawled_urls and frontier.exists_in_frontier(new_url) == False:\n",
        "              filtered_urls.append(new_url)\n",
        "            threadLock.release()\n",
        "          except:\n",
        "            filtered_urls.append(new_url)\n",
        "        \n",
        "    return filtered_urls\n",
        "\n",
        "  except:\n",
        "    return extracted_urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKQgGUb5K9lk"
      },
      "source": [
        "## ---------------------Write any other codes/data require to run the crawler above this cell-----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHDqDiJyK9ll"
      },
      "source": [
        "# Run Crawler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5KCNo-iK9lm"
      },
      "source": [
        "# Theard task\n",
        "\n",
        "def crawler_thread_task(thread_id, frontier):\n",
        "    threadLock.acquire()\n",
        "    url = frontier.get_URL()\n",
        "    threadLock.release()\n",
        "\n",
        "    while url == \"\":\n",
        "      threadLock.acquire()\n",
        "      url = frontier.get_URL()\n",
        "      threadLock.release()\n",
        "\n",
        "    print(\"Thread-\"+str(thread_id) + \"\\nCrawling: \" + url + \"\\n\")\n",
        "    reqs = requests.get(url)\n",
        "    crawled_urls.append(url)\n",
        "\n",
        "    threadLock.acquire()\n",
        "    records.insert_one({'url': url, 'content': reqs.text})\n",
        "    threadLock.release()\n",
        "\n",
        "    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
        "    extracted_urls = []\n",
        "    for link in soup.find_all('a'):\n",
        "        if link.get('href') != None and link.get(\"href\") != \"\" and (link.get('href')[0] == 'h' or link.get('href')[0] == '#' or link.get('href')[0] == '/'):\n",
        "          extracted_urls.append(link.get('href'))\n",
        "\n",
        "      \n",
        "    filtered_urls = filter_urls(url, extracted_urls, frontier)\n",
        "\n",
        "    threadLock.acquire()\n",
        "    frontier.add_URLs(filtered_urls)\n",
        "    threadLock.release()\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47B58tkiK9lo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "710f2078-d5be-4842-be62-70050816c7b7"
      },
      "source": [
        "# intialize every thing \n",
        "urls = [\"https://docs.oracle.com/en/\", \"https://www.oracle.com/corporate/\", \"https://en.wikipedia.org/wiki/Machine_learning\", \"https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html\", \"https://docs.oracle.com/middleware/jet210/jet/index.html\", \"https://en.wikipedia.org/w/api.php\", \"https://en.wikipedia.org/api/\", \"https://en.wikipedia.org/wiki/Weka_(machine_learning)\"]\n",
        "f = frontier(urls)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FRONT QUEUES: \n",
            "\n",
            "[]\n",
            "['https://docs.oracle.com/en/']\n",
            "['https://en.wikipedia.org/w/api.php']\n",
            "[]\n",
            "[]\n",
            "\n",
            "BACK QUEUES: \n",
            "\n",
            "['https://www.oracle.com/corporate/', 'https://docs.oracle.com/middleware/jet210/jet/index.html']\n",
            "['https://en.wikipedia.org/wiki/Machine_learning', 'https://en.wikipedia.org/api/', 'https://en.wikipedia.org/wiki/Weka_(machine_learning)']\n",
            "['https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5PzENw-K9lp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0599ddc-494f-40fa-e4a7-901a5ac758bf"
      },
      "source": [
        "# start the threads\n",
        "while len(crawled_urls) < 100:\n",
        "  threads = []\n",
        "  for i in range(THREADS):\n",
        "    threads.append(threading.Thread( target= crawler_thread_task, args=(i+1,f, ) ))\n",
        "\n",
        "  for i in range(THREADS):\n",
        "    threads[i].start()\n",
        "\n",
        "  for i in range(THREADS):\n",
        "    threads[i].join()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thread-1\n",
            "Crawling: https://www.oracle.com/corporate/\n",
            "\n",
            "Thread-2\n",
            "Crawling: https://en.wikipedia.org/wiki/Machine_learning\n",
            "\n",
            "Thread-3\n",
            "Crawling: https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html\n",
            "\n",
            "Thread-4\n",
            "Crawling: https://docs.oracle.com/middleware/jet210/jet/index.html\n",
            "Thread-5\n",
            "Crawling: https://en.wikipedia.org/api/\n",
            "\n",
            "\n",
            "Thread-6\n",
            "Crawling: https://docs.oracle.com/en/\n",
            "Thread-7\n",
            "Crawling: https://en.wikipedia.org/wiki/Weka_(machine_learning)\n",
            "\n",
            "\n",
            "Thread-8\n",
            "Crawling: https://en.wikipedia.org/w/api.php\n",
            "\n",
            "Thread-9\n",
            "Crawling: https://foundation.wikimedia.org/wiki/Developer_app_guidelines\n",
            "\n",
            "Thread-1\n",
            "Crawling: http://oraclejet.org\n",
            "Thread-2\n",
            "Crawling: https://docs.oracle.com/en/middleware/\n",
            "\n",
            "\n",
            "Thread-3\n",
            "Crawling: https://twitter.com/oracle\n",
            "\n",
            "Thread-4\n",
            "Crawling: https://meta.wikimedia.org\n",
            "\n",
            "Thread-5\n",
            "Crawling: http://www.oracle.com/webfolder/technetwork/jet/globalSupport-FAQ.html\n",
            "\n",
            "Thread-6\n",
            "Crawling: https://phabricator.wikimedia.org/maniphest/query/GebfyV4uCaLd/#R\n",
            "\n",
            "Thread-7\n",
            "Crawling: https://www.oracle.com/corporate/accessibility/\n",
            "\n",
            "Thread-8\n",
            "Crawling: https://phabricator.wikimedia.org/\n",
            "\n",
            "Thread-9\n",
            "Crawling: https://www.oracle.com/corporate/#search\n",
            "\n",
            "Thread-1\n",
            "Crawling: https://meta.wikimedia.org/wiki/Wikimedia_Apps/Developing_an_official_app\n",
            "Thread-2\n",
            "Crawling: https://www.oracle.com/universal-menu/\n",
            "\n",
            "Thread-3\n",
            "Crawling: https://meta.wikimedia.org/wiki/Wikimedia_Apps/Developing_an_official_app\n",
            "\n",
            "\n",
            "Thread-4\n",
            "Crawling: https://www.oracle.com/corporate/#search\n",
            "\n",
            "Thread-5\n",
            "Crawling: https://www.youtube.com/oracle/\n",
            "Thread-6\n",
            "Crawling: https://foundation.wikimedia.org/wiki/Trademark_policy\n",
            "\n",
            "\n",
            "Thread-7\n",
            "Crawling: https://www.oracle.com/corporate/#resources\n",
            "\n",
            "Thread-8\n",
            "Crawling: https://foundation.wikimedia.org/wiki/Developer_app_guidelines#searchInput\n",
            "Thread-9\n",
            "Crawling: https://www.oracle.com/corporate/#maincontent\n",
            "\n",
            "\n",
            "Thread-1\n",
            "Crawling: https://lists.wikimedia.org/mailman/listinfo/mediawiki-api\n",
            "\n",
            "Thread-2\n",
            "Crawling: https://www.oracle.com/cloud/sign-in.html\n",
            "\n",
            "Thread-3\n",
            "Crawling: https://lists.wikimedia.org/mailman/listinfo/mediawiki-api-announce\n",
            "\n",
            "Thread-4\n",
            "Crawling: https://www.oracle.com/\n",
            "\n",
            "Thread-5\n",
            "Crawling: https://en.wikipedia.org/wiki/en:Tablet_computer\n",
            "\n",
            "Thread-6\n",
            "Crawling: https://meta.wikimedia.org/wiki/Developer_app_guidelines/es\n",
            "\n",
            "Thread-7\n",
            "Crawling: https://www.oracle.com/corporate/#events\n",
            "\n",
            "Thread-8\n",
            "Crawling: https://en.wikipedia.org\n",
            "\n",
            "Thread-9\n",
            "Crawling: https://foundation.wikimedia.org/wiki/Wikimedia_trademarks\n",
            "\n",
            "Thread-1\n",
            "Crawling: https://www.oracle.com/corporate/#developer\n",
            "\n",
            "Thread-2\n",
            "Crawling: https://en.wikipedia.org/wiki/en:Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\n",
            "\n",
            "Thread-3\n",
            "Crawling: https://foundation.wikimedia.org/wiki/File:Mobile_App_Guidelines_-_confusing_logos_03.png\n",
            "Thread-4\n",
            "Crawling: https://www.oracle.com/corporate/#industries\n",
            "\n",
            "\n",
            "Thread-5\n",
            "Crawling: https://en.wikipedia.org/wiki/en:Smartphone\n",
            "Thread-6\n",
            "Crawling: https://foundation.wikimedia.org/wiki/File:Wikipedia-logo-v2.svg\n",
            "\n",
            "\n",
            "Thread-7\n",
            "Crawling: https://www.oracle.com/webapps/redirect/signon?nexturl=\n",
            "\n",
            "Thread-8\n",
            "Crawling: https://en.wikipedia.org/wiki/en:Wikipedia:Text_of_the_GNU_Free_Documentation_License\n",
            "\n",
            "Thread-9\n",
            "Crawling: https://foundation.wikimedia.org/wiki/File:Wikipedia%27s_W.svg\n",
            "\n",
            "Thread-1\n",
            "Crawling: https://www.oracle.com/corporate/#back\n",
            "Thread-2\n",
            "Crawling: https://en.wikipedia.org/wiki/en:Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\n",
            "\n",
            "\n",
            "Thread-3\n",
            "Crawling: https://commons.wikimedia.org/wiki/Commons:Reusing_content_outside_Wikimedia\n",
            "Thread-4\n",
            "Crawling: https://www.oracle.com/cloud/free/?source=:ow:o:h:nav:OHP0625ViewAccountsButton&intcmp=:ow:o:h:nav:OHP0625ViewAccountsButton\n",
            "\n",
            "\n",
            "Thread-5\n",
            "Crawling: https://en.wikipedia.org/wiki/Wikipedia:Reusing_Wikipedia_content\n",
            "\n",
            "Thread-6\n",
            "Crawling: https://www.oracle.com/corporate/careers/benefits/training.html\n",
            "\n",
            "Thread-7\n",
            "Crawling: https://en.wikipedia.org/wiki/en:Mobile_devices\n",
            "\n",
            "Thread-8\n",
            "Crawling: https://www.oracle.com/corporate/#products\n",
            "\n",
            "Thread-9\n",
            "Crawling: https://en.wikipedia.org/wiki/en:Wearable_computer\n",
            "\n",
            "Thread-1\n",
            "Crawling: https://www.oracle.com/corporate/contact/help.html\n",
            "\n",
            "Thread-2\n",
            "Crawling: https://en.wikipedia.org/wiki/en:Wikipedia:Copyrights#Reusers.27_rights_and_obligations\n",
            "\n",
            "Thread-3\n",
            "Crawling: https://www.oracle.com/corporate/careers/culture/who-we-are.html\n",
            "\n",
            "Thread-4\n",
            "Crawling: https://academy.oracle.com/en/oa-web-overview.html\n",
            "\n",
            "Thread-5\n",
            "Crawling: https://www.oracle.com/legal/privacy/marketing-cloud-data-cloud-privacy-policy.html#adchoices\n",
            "\n",
            "Thread-6\n",
            "Crawling: https://www.oracle.com/corporate/careers/\n",
            "Thread-7\n",
            "Crawling: https://www.oracle.com/opn/index.html\n",
            "\n",
            "\n",
            "Thread-8\n",
            "Crawling: https://www.oracle.com/corporate/contact/\n",
            "Thread-9\n",
            "Crawling: https://www.oracle.com/blockchain/\n",
            "\n",
            "\n",
            "Thread-1\n",
            "Crawling: https://www.oracle.com/corporate/#close\n",
            "Thread-2\n",
            "Crawling: https://www.oracle.com/corporate/executives/\n",
            "\n",
            "Thread-3\n",
            "Crawling: https://developer.oracle.com/\n",
            "\n",
            "\n",
            "Thread-4\n",
            "Crawling: https://www.oracle.com/javadownload\n",
            "\n",
            "Thread-5\n",
            "Crawling: https://go.oracle.com/subscriptions\n",
            "\n",
            "Thread-6\n",
            "Crawling: https://openclipart.org/image/800px/svg_to_png/171273/papillon.png\n",
            "\n",
            "Thread-7\n",
            "Crawling: https://profile.oracle.com/myprofile/account/create-account.jspx\n",
            "\n",
            "Thread-8\n",
            "Crawling: https://www.oracle.com/corporate/citizenship/\n",
            "\n",
            "Thread-9\n",
            "Crawling: https://partner-finder.oracle.com/\n",
            "\n",
            "Thread-1\n",
            "Crawling: https://www.oracle.com/universal-menu/index.html#u10countrymenu\n",
            "\n",
            "Thread-2\n",
            "Crawling: https://www.oracle.com/partnernetwork/\n",
            "\n",
            "Thread-3\n",
            "Crawling: https://www.oracle.com/legal/copyright.html\n",
            "Thread-4\n",
            "Crawling: https://foundation.wikimedia.org/wiki/Mission_statement\n",
            "\n",
            "\n",
            "Thread-5\n",
            "Crawling: https://www.oracle.com/corporate/covid-19.html\n",
            "\n",
            "Thread-6\n",
            "Crawling: https://www.blogger.com/profile/15793594885891938322\n",
            "\n",
            "Thread-7\n",
            "Crawling: https://foundation.wikimedia.org/wiki/Wikimedia_trademarks\n",
            "Thread-8\n",
            "Crawling: https://www.oracle.com/cloud/free/?source=:ow:o:h:nav:050120SiteFooter&intcmp=:ow:o:h:nav:050120SiteFooter\n",
            "\n",
            "\n",
            "Thread-9\n",
            "Crawling: https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\n",
            "\n",
            "Thread-1\n",
            "Crawling: https://foundation.wikimedia.org/wiki/Category:Governance_wiki\n",
            "\n",
            "Thread-2\n",
            "Crawling: https://www.oracle.com/legal/privacy/\n",
            "\n",
            "Thread-3\n",
            "Crawling: https://en.wikipedia.org/wiki/Wikipedia:Reusing_Wikipedia_content\n",
            "\n",
            "Thread-4\n",
            "Crawling: https://foundation.wikimedia.org/wiki/Home\n",
            "Thread-5\n",
            "Crawling: https://wikimediafoundation.org/\n",
            "\n",
            "\n",
            "Thread-6\n",
            "Crawling: https://www.oracle.com/legal/privacy/privacy-choices.html\n",
            "\n",
            "Thread-7\n",
            "Crawling: https://foundation.wikimedia.org/wiki/Wikimedia_trademarks\n",
            "\n",
            "Thread-8\n",
            "Crawling: https://wikimediafoundation.org/news/\n",
            "\n",
            "Thread-9\n",
            "Crawling: https://www.oracle.com/sitemap.html\n",
            "\n",
            "Thread-1\n",
            "Crawling: https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License\n",
            "\n",
            "Thread-2\n",
            "Crawling: https://foundation.wikimedia.org/wiki/Wikimedia_trademarks\n",
            "\n",
            "Thread-3\n",
            "Crawling: https://www.oracle.com/corporate/#support\n",
            "\n",
            "Thread-4\n",
            "Crawling: https://en.wikipedia.org/wiki/Wikipedia:Reusing_Wikipedia_content\n",
            "\n",
            "Thread-5\n",
            "Crawling: https://foundation.wikimedia.org/wiki/Developer_app_guidelines#top\n",
            "\n",
            "Thread-6\n",
            "Crawling: https://www.oracle.com/startup/\n",
            "Thread-7\n",
            "Crawling: https://en.wikipedia.org/wiki/Software_developer\n",
            "\n",
            "\n",
            "Thread-8\n",
            "Crawling: https://store.wikimedia.org\n",
            "\n",
            "Thread-9\n",
            "Crawling: https://www.oracle.com/corporate/careers/\n",
            "\n",
            "Thread-1\n",
            "Crawling: https://en.wikipedia.org/wiki/Windows\n",
            "\n",
            "Thread-2\n",
            "Crawling: https://www.mediawiki.org/wiki/API:Imageinfo\n",
            "\n",
            "Thread-3\n",
            "Crawling: https://annual.wikimedia.org/\n",
            "Thread-4\n",
            "Crawling: https://wikimediafoundation.org/participate/\n",
            "\n",
            "\n",
            "Thread-5\n",
            "Crawling: https://en.wikipedia.org/wiki/IA-32\n",
            "Thread-6\n",
            "Crawling: https://www.oracle.com/corporate/careers/culture/diversity.html\n",
            "\n",
            "\n",
            "Thread-7\n",
            "Crawling: https://foundation.wikimedia.org/wiki/File:Mobile_App_Guidelines_-_not_confusing_logos_01.png\n",
            "\n",
            "Thread-8\n",
            "Crawling: https://en.wikipedia.org/wiki/Weka_(machine_learning)#cite_note-:0-1\n",
            "Thread-9\n",
            "Crawling: https://www.oracle.com/startup/\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DxoEowDK9lr"
      },
      "source": [
        "## ------------------------------------------------------End of Notebook---------------------------------------------------"
      ]
    }
  ]
}